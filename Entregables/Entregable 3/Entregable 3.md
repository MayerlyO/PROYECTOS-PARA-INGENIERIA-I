<p align="left">
  <img src="https://upchvirtual.edu.pe/ued/images/logo-upch.png" width="200">

# _INFORME 03: Tiny Machine Learning con Edge Impulseüìë_

## Docentes:üë®‚Äçüè´
- Mg Umbert Lewis De La Cruz
- Mg. Paulo Camilo Vela Ant√≥n
- Mg. Moises Stevend Meza Rodriguez
- Dr. Harry Anderson Rivera Tito
- Ing. Juan Manuel Zu√±iga Mamani
- Ing. Renzo Jos√© Chan R√≠os
## Integrantesüìä
- Flores Huaman, Meyli 
- Huarca Astete, Iory Estefani
- Mendoza Canaza, Marco Antonio
- Orosco Taype, Mayerly Nicole

### 1. Introducci√≥n:
El aprendizaje autom√°tico (ML) ha tenido un impacto significativo en varios campos, incluidos la visi√≥n, el lenguaje y el audio, puesto que permite ejecutar modelos avanzados de inteligencia artificial en dispositivos embebidos y de bajo consumo energ√©tico [1]. Con Edge Impulse, una plataforma que facilita la creaci√≥n y despliegue de estos modelos, podemos entrenar sistemas para realizar tareas complejas, integr√°ndose en el hardware m√°s simple.
En el presente proyecto, utilizaremos un Arduino Nano 33 BLE Sense para identificar comandos de gestos. Aprovechando los algoritmos optimizados por Edge Impulse, donde podremos capturar y procesar gestos en tiempo real, ejecutando acciones con 2 movimientos . Este peque√±o dispositivo ser√° capaz de comprender el entorno y reaccionar de manera inteligente, todo gracias a la potencia de TinyML y la flexibilidad de Edge Impulse [4].<br>

La integraci√≥n de este modelo en el Arduino Nano 33 BLE Sense se basa en tres condiciones espec√≠ficas:

- **LED Rojo:** Se enciende al dibujar un c√≠rculo.
- **LED Azul:** Se activa al representar el n√∫mero 3.
- **LED Verde:** Se ilumina al trazar el n√∫mero 1.

Cada una de estas acciones desencadena una respuesta luminosa en el dispositivo, creando una interacci√≥n √∫nica y visualmente atractiva.
A lo largo de este informe se detallar√°n los procedimientos de desarrollo, desde la recolecci√≥n de datos hasta la implementaci√≥n del modelo en el hardware, as√≠ como los resultados obtenidos y las conclusiones alcanzadas. 



### 2. Metodolog√≠a:
En este apartado, explicaremos a detalle el proceso que seguimos para cumplir con el reto planteado. 

#### 2.1 Obtenci√≥n y preparaci√≥n de los datos:
- Dispositivos usados: Se utiliz√≥ un Arduino Nano 33 BLE Sense, que tiene sensores como el aceler√≥metro (acc), el giroscopio (gyr) y el magnet√≥metro (mag), para capturar los movimientos que se asocian a los gestos de dibujo (c√≠rculo, n√∫mero 1 y  n√∫mero 3).
<table>
  <tr>
    <th width="50%"><img src="https://github.com/MayerlyO/PROYECTOS-PARA-INGENIERIA-I/blob/main/Entregables/Entregable%203/Im%C3%A1genes/arduino_nano_33_ble.jpg" width="280" height="280"> <br> Arduino NANO 33 BLE Sense [Fotograf√≠a]. (s.f.)</th>
    <th width="50%" align="justify" >El Arduino Nano 33 BLE es una placa compacta y potente ideal para proyectos de Internet de las Cosas (IoT). Con un microcontrolador y conectividad Bluetooth, lo que permite una comunicaci√≥n eficiente. Este incluye sensores como un aceler√≥metro y un giroscopio. Su f√°cil programaci√≥n en el entorno de Arduino lo hace accesible. En este caso lo emplearemos para reconocimiento de se√±ales y recibiremos una interacci√≥n luminosa.</th>
  </tr>
</table><br>

- Serie de tiempo: Como se observa en la sigueinte imagen, los datos de la serie de tiempo capturados incluyen 9 ejes de datos (accX, accY, accZ, gyrX, gyrY, gyrZ, magX, magY, magZ), que miden diferentes aspectos del movimiento y la orientaci√≥n del dispositivo.De igual modo se puede observar una secci√≥n  donde se logr√≥ definir el tama√±o de ventana de 3010 ms con un incremento de ventana de 200 ms. Esto significa que los datos se recogen en intervalos de 3 segundos con una frecuencia de muestreo de 100 Hz.
<p align="center">
  <img src="https://github.com/MayerlyO/PROYECTOS-PARA-INGENIERIA-I/blob/main/Entregables/Entregable%203/Im%C3%A1genes/E3_1.jpg"width="750"> <br><br>
  
#### 2.2  Comportamiento espectral (Procesamiento de datos):
El an√°lisis espectral nos permiti√≥ extraer caracter√≠sticas √∫tiles de los datos en bruto. Al marcar todas las entradas disponibles, utilizando informaci√≥n tanto del aceler√≥metro como del giroscopio y el magnet√≥metro. Estos datos transformados en caracter√≠sticas espectrales ayudan a que el modelo capture patrones repetitivos en los gestos para luego emitir un color de acuerdo al gesto realizado.
<p align="center">
  <img src="https://github.com/MayerlyO/PROYECTOS-PARA-INGENIERIA-I/blob/main/Entregables/Entregable%203/Im%C3%A1genes/E3_2.jpg"width="750"> <br><br>

#### 2.3 Clasificaci√≥n de gestos:
- El modelo tiene tres salidas: un c√≠rculo ("O"), el n√∫mero 3 y el n√∫mero 1. Este clasificador utiliza las caracter√≠sticas espectrales de las se√±ales para identificar qu√© gesto se est√° realizando y emitir un color.
- Como muestra la segunda imagen, la divisi√≥n de datos entre entrenamiento y prueba es del 80% para entrenar el modelo y el 20% para evaluarlo, asegurando que el modelo tenga datos suficientes para aprender sin sobre ajustarse.
<p align="center">
  <img src="https://github.com/MayerlyO/PROYECTOS-PARA-INGENIERIA-I/blob/main/Entregables/Entregable%203/Im%C3%A1genes/E3_3.jpg"width="750"> <br><br>

#### 2.4 Entrenamiento y pruebas:
- En el segundo conjunto de im√°genes, se puede ver c√≥mo se etiquetaron y organizaron los datos para entrenamiento y prueba. Cada gesto capturado (c√≠rculo, 1, 3) est√° representado por muestras etiquetadas (como el ejemplo "1.57bq53rd") que contienen las series de datos de los sensores. 
- Las series de tiempo capturadas se muestran gr√°ficamente para cada una de las etiquetas, lo que permite visualizar los patrones de movimiento caracter√≠sticos de cada gesto. <br><br>

#### 2.5 Implementaci√≥n en Arduino (TinyML):
- Una vez que el modelo ha sido entrenado y validado, se exporta el modelo optimizado para que se ejecute en el Arduino Nano 33 BLE Sense. Este paso implica compilar el modelo para un entorno TinyML y cargar el c√≥digo resultante en el microcontrolador.
<p align="center">
  <img src="https://github.com/MayerlyO/PROYECTOS-PARA-INGENIERIA-I/blob/main/Entregables/Entregable%203/Im%C3%A1genes/E3_4.jpg"width="450"> <br><br>

### 3. Resultados:
Pasamos a probar el modelo realizado en Edge Impulse para comprobar si este predice correctamente las figuras que estamos dibujando con el ARDUINO NANO 33 BLE SENSE. <br><br>
#### 3.1 Video prueba del TRES



### 4. Discusi√≥n:
En la actualidad, la inteligencia artificial se puede encontrar cada vez en un mayor n√∫mero de dispositivos sin que los usuarios sean conscientes de ello, un ejemplo es el uso de una palabra o gesto para activar un dispositivo y ejecutar acciones m√°s complejas, puesto que el aprendizaje autom√°tico ha revolucionado nuestra forma de interactuar con la tecnolog√≠a, especialmente en el √°mbito de los dispositivos embebidos [2]. En este reto, trabajamos con el Arduino Nano 33 BLE Sense, experimentamos de primera mano c√≥mo esta placa puede ejecutar modelos de inteligencia artificial para reconocer comandos de gestos. Sin embargo, no todo fue un camino f√°cil. La recolecci√≥n y preparaci√≥n de datos fue un desaf√≠o significativo; tuvimos que asegurarnos de que los datos fueran representativos y bien etiquetados, lo que requiri√≥ mucho tiempo y esfuerzo en equipo. 
<br><br>
Resulta importante destacar un estudio realizado por Viswanatha V,  Ramachandra AC y otros investigadores en el a√±o 2022, que se enfoca en el reconocimiento de gestos, puesto que nos proporciona un enfoque innovador de la comunicaci√≥n no verbal ya que tiene amplias aplicaciones en la interacci√≥n hombre-m√°quina y en el lenguaje de se√±as, en la implementaci√≥n del reconocimiento de gestos de la mano, el modelo Tiny ML se entrena y se implementa desde el marco Edge Impulse para el reconocimiento de gestos de la mano y, en funci√≥n de los movimientos de la mano, el dispositivo Arduino Nano 33 BLE con IMU de 6 ejes puede averiguar la direcci√≥n del movimiento de la mano la integraci√≥n de retroalimentaci√≥n visual, como el uso de LEDs para indicar acciones, las cuales se puede poner en pr√°ctica en los diversos contextos o en la solucion de problematicas de nuestro pa√≠s [3]. 
<br><br>
Esta experiencia enriqueci√≥ nuestra interacci√≥n con el Arduino, ya que ver c√≥mo el dispositivo respond√≠a a nuestros comandos, a pesar de algunas limitaciones, fue gratificante. Esta retroalimentaci√≥n positiva nos inspir√≥ a ajustar nuestro enfoque y a explorar nuevas t√©cnicas, fortaleciendo nuestro compromiso con el aprendizaje y la innovaci√≥n.<br><br>

### 5. Conclusi√≥n:
En conclusi√≥n, nuestra experiencia con el Arduino Nano 33 BLE Sense y el aprendizaje autom√°tico ha sido un viaje lleno de desaf√≠os y aprendizajes significativos. La integraci√≥n de retroalimentaci√≥n visual no solo mejor√≥ la interacci√≥n, sino que tambi√©n nos motiv√≥ a seguir refinando nuestro enfoque. Estas experiencias nos han ense√±ado la importancia del trabajo en equipo, la adaptabilidad y la perseverancia. Con estas lecciones en mente, nos sentimos m√°s preparados para enfrentar futuros retos y continuar explorando las posibilidades del aprendizaje autom√°tico en dispositivos embebidos, con la confianza de que cada obst√°culo superado nos acerca m√°s a nuestras metas.<br><br>

### 6. Bibliograf√≠a:
1. Capogrosso, L., Cunico, F., Cheng, DS, Fummi, F. y Cristani, M. (2024). Una encuesta orientada al aprendizaje autom√°tico sobre el aprendizaje autom√°tico en miniatura. IEEE Access . 
2. Cebollo, R. A., & del Br√≠o, B. M. Reconocimiento de gestos y voz mediante inteligencia artificial incorporada en un microcontrolador.
3. Prasanna, R., Kakarla, P. C., PJ, V. S., & Mohan, N. (2022). Implementation of tiny machine learning models on arduino 33 ble for gesture and speech recognition. arXiv preprint arXiv:2207.12866.
4. B. V. Nived, K. Jamal, G. Mahesh and R. M. Kumar, "Design of Custom Keyword Recognition using Edge Impulse on Arduino Nano 33 BLE Sense," 2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC), Salem, India, 2023, pp. 1522-1529, doi: 10.1109/ICAAIC56838.2023.10140757. keywords: {Microcontrollers;Neural networks;Random access memory;Speech recognition;Feature extraction;Hardware;Data models;Voice Recognition;Sampling and Framing;MFCC;Classification;Neural Networks;Edge Impulse},

